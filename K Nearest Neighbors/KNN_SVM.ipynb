{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:08:03.992588Z",
     "start_time": "2019-10-28T16:07:55.333760Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features 100\n",
      "Num Obs: 2000\n",
      "Count of Postive Class labels: 1000\n",
      "Count of Negative Class labels: 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-91a2481fe248>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;31m# Task 2 : implement 4 fold cross validation to select best k from klist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m \u001b[0mfindBestK\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mklist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Testing with Sklearn, neighbors = 11'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-91a2481fe248>\u001b[0m in \u001b[0;36mfindBestK\u001b[1;34m(x, y, klist, nfolds)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfolds\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mfold_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-91a2481fe248>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(x_train, y_train, x_test, k)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0msorted_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mneighbors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-91a2481fe248>\u001b[0m in \u001b[0;36meuclidean_distance\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m# 3. Classify each testing points based on the training points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[0m\u001b[0;32m     71\u001b[0m                   if v is not np._NoValue}\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Starting code for UVA CS 4501 Machine Learning- KNN\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(37)\n",
    "# for plot\n",
    "import matplotlib.pyplot as plt\n",
    "#more imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "## the only purpose of the above import is in case that you want to compare your knn with sklearn knn\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "# Load file into np arrays\n",
    "# x is the features\n",
    "# y is the labels\n",
    "def read_file(file):\n",
    "    data = np.loadtxt(file, skiprows=1)\n",
    "    np.random.shuffle(data)\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1].astype(int)\n",
    "    return x, y\n",
    "\n",
    "# 2. Generate the i-th fold of k fold validation\n",
    "# Input:\n",
    "# x is an np array for training data\n",
    "# y is an np array for labels\n",
    "# i is an int indicating current fold\n",
    "# nfolds is the total number of cross validation folds\n",
    "\n",
    "def fold(x, y ,i, nfolds):\n",
    "    valid_losses = []\n",
    "    training_losses = []\n",
    "    k = nfolds\n",
    "    #try catching for remainder/uneven split\n",
    "    m = len(x)\n",
    "    extras = m % nfolds #get remainder of length of set divided by number of folds\n",
    "    \n",
    "    if extras != 0:\n",
    "        x_train_extras = x[-extras:] #store for later\n",
    "        y_train_extras = x[-extras:] #store for later\n",
    "        \n",
    "        x_train = y[:-extras] #discard extras\n",
    "        y_train = y[:-extras]#discard extras\n",
    "        \n",
    "    #split the data into k equal sized array\n",
    "    x_train_split = np.split(x, k)\n",
    "    y_train_split = np.split(y, k)\n",
    "    \n",
    "    sets = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        #loop through k folds\n",
    "        #take a split of the data for testing\n",
    "        x_test, y_test = x_train_split[i], y_train_split[i]\n",
    "        #take the remaining 3/4 of the data for training\n",
    "        x_train = np.concatenate(x_train_split[:i] + x_train_split[i + 1:], axis=0)\n",
    "        y_train = np.concatenate(y_train_split[:i] + y_train_split[i + 1:], axis=0)\n",
    "        sets.append([x_train, x_test, y_train, y_test])\n",
    "        \n",
    "    #try catching for remainder/uneven split. adding to last set as training example\n",
    "    if extras != 0:\n",
    "        np.append(sets[-1][0], x_train_extras, axis=0)\n",
    "        np.append(sets[-1][2], y_train_extras, axis=0)\n",
    "        \n",
    "    return sets[i]\n",
    "\n",
    "def euclidean_distance(x,y):\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "    \n",
    "# 3. Classify each testing points based on the training points\n",
    "# Input\n",
    "# x_train: a numpy array of training data \n",
    "# x_test: a numpy array\n",
    "# k: the number of neighbors to take into account when predicting the label\n",
    "# Output\n",
    "# y_predict: a numpy array \n",
    "def classify(x_train, y_train, x_test, k):\n",
    "    neighbors = {}\n",
    "    for i in range(len(x_test)):\n",
    "        distances = [] \n",
    "        for j in range(len(x_train)):\n",
    "            distances.append((j, euclidean_distance(x_test[i], x_train[j]), y_train[j]))\n",
    "        sorted_all = sorted(distances, key = lambda kv: kv[1])\n",
    "        neighbors[i] = sorted_all[:k]\n",
    "        \n",
    "    y_predict = []\n",
    "    for i in range(len(x_test)):\n",
    "        neighbors_y = Counter([x[2] for x in neighbors[i]])\n",
    "        y_predict.append(max(neighbors_y.items(), key=operator.itemgetter(1))[0])\n",
    "    # your code\n",
    "    # Euclidean distance as the measurement of distance in KNN\n",
    "    return np.array(y_predict)\n",
    "\n",
    "# 4. Calculate accuracy by comaring with true labels\n",
    "# Input\n",
    "# y_predict is a numpy array of 1s and 0s for the class prediction\n",
    "# y is a numpy array of 1s and 0s for the true class label\n",
    "def calc_accuracy(y_predict, y_true):\n",
    "    acc = np.sum(y_true == y_predict, axis=0) / len(y_true)\n",
    "    return acc\n",
    "\n",
    "# 5. Draw the bar plot of k vs. accuracy\n",
    "# klist: a list of values of ks\n",
    "# accuracy_list: a list of accuracies\n",
    "def barplot(klist, accuracy_list):\n",
    "    plt.bar(klist, accuracy_list)\n",
    "    plt.title('# of Neighbors Vs CV Accuracy')\n",
    "    plt.xlabel('# of neighbors')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# 1. Find the best K\n",
    "def findBestK(x, y, klist, nfolds):\n",
    "    kbest = 0\n",
    "    best_acc = 0\n",
    "    counter = 0\n",
    "    accuracy_list = []\n",
    "    for k in klist:\n",
    "        fold_acc = []\n",
    "        for i in range(0, nfolds - 1):\n",
    "            x_train, x_test, y_train, y_test = fold(x,y, i, nfolds)\n",
    "            y_predict = classify(x_train, y_train, x_test, k=k)\n",
    "            accuracy = calc_accuracy(y_predict, y_test)\n",
    "            fold_acc.append(accuracy)\n",
    "            \n",
    "        accuracy_list.append(np.mean(accuracy))\n",
    "        accuracy = accuracy_list[counter]\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            kbest = k\n",
    "            best_acc = accuracy\n",
    "        print('Neighbors: {}, cv accuracy {}'.format(k, accuracy))\n",
    "        \n",
    "        counter += 1\n",
    "    # plot cross validation error for each k : implement function barplot(klist, accuracy_list)\n",
    "    barplot(klist, accuracy_list)\n",
    "    return kbest\n",
    "\n",
    "filename = \"Movie_Review_Data.txt\"\n",
    "# read data\n",
    "x, y = read_file(filename)\n",
    "print('Num Features', x.shape[1])\n",
    "print('Num Obs:', x.shape[0])\n",
    "print('Count of Postive Class labels:', Counter(y)[1])\n",
    "print('Count of Negative Class labels:', Counter(y)[0])\n",
    "nfolds = 8\n",
    "klist = [3, 5,7, 9, 11, 13]\n",
    "# Implementation covers two tasks, both part of findBestK function\n",
    "# Task 1 : implement kNN classifier for a given x,y,k\n",
    "# Task 2 : implement 4 fold cross validation to select best k from klist\n",
    "\n",
    "findBestK(x, y, klist, nfolds)\n",
    "print('-' *25)\n",
    "print('Testing with Sklearn, neighbors = 11')\n",
    "model = KNeighborsClassifier( 11)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "acc = calc_accuracy(y_pred, y_test)\n",
    "print('sklearn KNN accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T14:09:36.579139Z",
     "start_time": "2019-10-14T14:09:36.575152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTesting decision boundaries on IRIS with various C values\\n\\n\\n\\nfrom sklearn.svm import SVC\\nimport matplotlib.pyplot as plt\\nfrom mlxtend.plotting import plot_decision_regions\\nfrom sklearn import svm, datasets\\n\\niris = datasets.load_iris()\\n\\nX = iris.data[:, :2]\\ny = iris.target\\n\\nfor i in [1e-10, 1e-1,1e1, 1e10]:\\n    svm = SVC(C=i, kernel='rbf')\\n    svm.fit(X, y)\\n    plot_decision_regions(X, y, clf=svm, legend=2)\\n    plt.title('C:{}'.format(i))\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Testing decision boundaries on IRIS with various C values\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "for i in [1e-10, 1e-1,1e1, 1e10]:\n",
    "    svm = SVC(C=i, kernel='rbf')\n",
    "    svm.fit(X, y)\n",
    "    plot_decision_regions(X, y, clf=svm, legend=2)\n",
    "    plt.title('C:{}'.format(i))\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:17:24.187411Z",
     "start_time": "2019-10-28T16:17:23.398522Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Starting code for UVA CS 4501 ML- SVM\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(37)\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Att: You're not allowed to use modules other than SVC in sklearn, i.e., model_selection.\n",
    "\n",
    "# Dataset information\n",
    "# the column names (names of the features) in the data files\n",
    "# you can use this information to preprocess the features\n",
    "col_names_x = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "             'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "             'hours-per-week', 'native-country']\n",
    "col_names_y = ['label']\n",
    "\n",
    "numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "                  'hours-per-week']\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship',\n",
    "                    'race', 'sex', 'native-country']\n",
    "\n",
    "\n",
    "# 1. Data loading from file and pre-processing.\n",
    "# Hint: Feel free to use some existing libraries for easier data pre-processing. \n",
    "# For example, as a start you can use one hot encoding for the categorical variables and normalization \n",
    "# for the continuous variables.\n",
    "\n",
    "        \n",
    "def calc_accuracy(y_predict, y_true):\n",
    "    acc = np.sum(y_true == y_predict, axis=0) / len(y_true) #count where reponses match divided by len of y_test\n",
    "    return acc\n",
    "        \n",
    "        \n",
    "    \n",
    "def load_data(csv_file_path, encode_cat = False, outlier_removal = True, feature_engineering = True, down_sample = False):\n",
    "    print('reading in data')\n",
    "    #I believe pandas is allowed. Using it for easier recognition of column names\n",
    "    data = pd.read_csv(csv_file_path, names = col_names_x + col_names_y) \n",
    "    df = data.copy()\n",
    "    df['label']  = df['label'].apply(lambda x: 0 if x==' <=50K' else 1)\n",
    "    #df['native-country']  = df['native-country'].apply(lambda x: \"Other\" if x!= \" United-States\" else \"United-States\")\n",
    "    df.drop(['native-country'], axis=1, inplace = True)\n",
    "    if feature_engineering:\n",
    "        df[\"marital-status\"] = df[\"marital-status\"].replace([' Married-civ-spouse',' Married-spouse-absent',' Married-AF-spouse'], 'Married')\n",
    "        df[\"marital-status\"] = df[\"marital-status\"].replace([' Never-married',' Divorced',' Separated',' Widowed'], 'Single')\n",
    "        df['race'] = df['race'].map({' White': 1, ' Asian-Pac-Islander': 1, ' Black':0, ' Amer-Indian-Eskimo':0, ' Other':0})\n",
    "        df['relationship'] = df['relationship'].map({' Not-in-family':0, ' Unmarried':0, ' Own-child':0, ' Other-relative':0, ' Husband':1, ' Wife':1})\n",
    "        #df.drop(categorical_cols, axis=1, inplace = True)\n",
    "    if outlier_removal:\n",
    "        print('removing outliers')\n",
    "        df = df[(np.abs(stats.zscore(df[numerical_cols])) < 3).all(axis=1)]\n",
    "\n",
    "    \n",
    "    if encode_cat: #encode cat features / get dummies\n",
    "        print('Categorical one hot encoding')\n",
    "        df = pd.get_dummies(df)\n",
    "\n",
    "    x = df.drop(['label'], axis=1) #extract ind. features\n",
    "    y = df['label']#extract response\n",
    "    if down_sample:\n",
    "        x = x[:10000]\n",
    "        y = y[:10000]\n",
    "    return x,y, x.columns\n",
    "\n",
    "def cross_validation(x_train, y_train, xcols, params, num_folds=3):\n",
    "    \n",
    "    #Split Data into k sets. 'Sets' contains set of sets\n",
    "    ######################################################################################\n",
    "    k = num_folds\n",
    "    #try catching for remainder/uneven split\n",
    "    m = len(x_train)\n",
    "    extras = m % num_folds  #get remainder of length of set divided by number of folds\n",
    "    if extras != 0:\n",
    "        x_train_extras = x_train[-extras:]\n",
    "        y_train_extras = y_train[-extras:]  #store for later\n",
    "        x_train = x_train[:-extras]  #discard extras\n",
    "        y_train = y_train[:-extras]  #discard extras\n",
    "    #print(len(x_train))\n",
    "    #print(len(y_train))\n",
    "    x_train_split = np.split(x_train, k)\n",
    "    y_train_split = np.split(y_train, k)\n",
    "    \n",
    "    sets = []\n",
    "    for i in range(k):\n",
    "        #loop through k folds\n",
    "        #take a split of the data for testing\n",
    "        X_test = pd.DataFrame(x_train_split[i],\n",
    "                              columns =xcols)\n",
    "        y_test = y_train_split[i]\n",
    "        #take the remaining 3/4 of the data for training\n",
    "        X_train = pd.DataFrame(np.concatenate(x_train_split[:i] +\n",
    "                                              x_train_split[i + 1:],\n",
    "                                              axis=0),\n",
    "                               columns =xcols)\n",
    "        y_train = np.concatenate(y_train_split[:i] +\n",
    "                                              y_train_split[i + 1:],\n",
    "                                              axis=0)\n",
    "        sets.append([X_train, X_test, y_train, y_test])\n",
    "    print('number of sets:', len(sets))\n",
    "    #try catching for remainder/uneven split. adding to last set as training example\n",
    "    #if extras != 0:\n",
    "    \n",
    "    #    np.append(sets[-1][0], x_train_extras, axis=0)\n",
    "    #    np.append(sets[-1][2], y_train_extras, axis=0)\n",
    "    ######################################################################################\n",
    "    #Grid search + KFOLD\n",
    "    ######################################################################################\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    models = []\n",
    "    counter = 0\n",
    "    for i in params:\n",
    "        temp_test_acc = []\n",
    "        temp_train_acc = []\n",
    "        model = SVC(C=i['C'],\n",
    "                    kernel=i['kernel'],\n",
    "                    degree=i['degree'],\n",
    "                    gamma='auto')\n",
    "        models.append(model)\n",
    "        fold = 1\n",
    "        print(i)\n",
    "        for j in sets:\n",
    "            scaler = MinMaxScaler().fit((j[0])) #fit minmax scaler to trainset\n",
    "            clean_x_train = pd.DataFrame(scaler.transform(j[0]), columns = xcols)  #scale\n",
    "            \n",
    "            model.fit(clean_x_train, j[2])  #fit on cleaned/scaled train set\n",
    "            \n",
    "            clean_x_test = pd.DataFrame(scaler.transform(j[1]), columns = xcols)  #clean/scale test set\n",
    "            \n",
    "            train_preds = model.predict(clean_x_train)\n",
    "            test_preds = model.predict(clean_x_test)  #predict on clean/scaled test set\n",
    "            \n",
    "            train_accuracy = calc_accuracy(j[-2], train_preds)\n",
    "            test_accuracy = calc_accuracy(j[-1], test_preds)\n",
    "            \n",
    "            temp_train_acc.append(train_accuracy)\n",
    "            temp_test_acc.append(test_accuracy)\n",
    "\n",
    "            fold += 1\n",
    "            \n",
    "        train_acc.append(np.mean(temp_train_acc))  #get mean over all folds\n",
    "        valid_acc.append(np.mean(temp_test_acc))  #get mean over all folds\n",
    "        print('mean accuracy on train set across folds:', train_acc[counter])\n",
    "        print('mean accuracy on holdout set across folds:', valid_acc[counter])\n",
    "        fold = 1\n",
    "        counter += 1\n",
    "    df = pd.DataFrame(params)\n",
    "    df['cv_train_accuracy'] = train_acc\n",
    "    df['cv_test_accuracy'] = valid_acc\n",
    "    best_params = params[df['cv_test_accuracy'].idxmax()]\n",
    "    return df, df['cv_test_accuracy'].max(), models[df['cv_test_accuracy'].idxmax()], best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:17:37.822474Z",
     "start_time": "2019-10-28T16:17:37.813465Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_saved = False\n",
    "submission = False\n",
    "\n",
    "# 2. Select best hyperparameter with cross validation and train model.\n",
    "# Attention: Write your own hyper-parameter candidates.\n",
    "def train_and_select_model(training_csv):\n",
    "    # load data and preprocess from filename training_csv\n",
    "    if submission:\n",
    "        x,y, xcols = load_data(training_csv, encode_cat = True, outlier_removal = True, \n",
    "                               feature_engineering = True, down_sample= True)\n",
    "        print('Turned in Version is downsampled to include only 10k observations')\n",
    "\n",
    "    else:\n",
    "        x,y, xcols = load_data(training_csv, encode_cat = True, outlier_removal = True, \n",
    "                               feature_engineering = True, down_sample= True)\n",
    "    # hard code hyperparameter configurations, an example:\n",
    "    param_set = [\n",
    "                     #{'kernel': 'rbf', 'C': .1, 'degree': 1},\n",
    "                    #{'kernel': 'rbf', 'C': 1, 'degree': 1},\n",
    "                     #{'kernel': 'rbf', 'C': 10, 'degree': 1},\n",
    "                    {'kernel': 'rbf', 'C': 100, 'degree': 1},\n",
    "                    #{'kernel': 'rbf', 'C': 1000, 'degree': 1},\n",
    "                    #{'kernel': 'rbf', 'C': 5000, 'degree': 1},\n",
    "        #{'kernel': 'rbf', 'C': 50, 'degree': 1},\n",
    "        #{'kernel': 'rbf', 'C': .001, 'degree': 1},\n",
    "        #{'kernel': 'rbf', 'C': .0001, 'degree': 1},\n",
    "\n",
    "        ]\n",
    "\n",
    "    table, best_score, best_model, params = cross_validation(x, y, xcols, param_set, num_folds= 3)\n",
    "    model = SVC(C=params['C'],kernel=params['kernel'],degree=params['degree'], gamma='auto')\n",
    "    best_model = model.fit(x,y)\n",
    "    \n",
    "    # your code here\n",
    "    # iterate over all hyperparameter configurations\n",
    "    # perform 3 FOLD cross validation\n",
    "    # print cv scores for every hyperparameter and include in pdf report\n",
    "    # select best hyperparameter from cv scores, retrain model \n",
    "    return best_model, best_score, x, table\n",
    "\n",
    "# predict for data in filename test_csv using trained model\n",
    "def predict(test_csv, trained_model, x):\n",
    "    x_test, _ , xcols = load_data(test_csv, encode_cat = True, outlier_removal = False, feature_engineering = True)\n",
    "    x_test = pd.DataFrame(MinMaxScaler().fit_transform(x_test) , columns = xcols)\n",
    "    _,x_test = x.align(x_test, join='outer', axis=1, fill_value=0)\n",
    "    xcols = list(x.columns)\n",
    "    xtestcols = list(x_test.columns)\n",
    "    x_test.drop([x for x in xtestcols if x not in xcols], axis = 1, inplace = True)\n",
    "    predictions = trained_model.predict(x_test)\n",
    "    return predictions\n",
    "\n",
    "# save predictions on test data in desired format \n",
    "def output_results(predictions):\n",
    "    with open('predictions.txt', 'w') as f:\n",
    "        for pred in predictions:\n",
    "            if pred == 0:\n",
    "                f.write('<=50K\\n')\n",
    "            else:\n",
    "                f.write('>50K\\n')\n",
    "    print('filed saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fill in train_and_select_model(training_csv) to \n",
    "# return a trained model with best hyperparameter from 3-FOLD \n",
    "# cross validation to select hyperparameters as well as cross validation score for best hyperparameter. \n",
    "# hardcode hyperparameter configurations as part of train_and_select_model(training_csv)\n",
    "trained_model, cv_score, x, _ = train_and_select_model(training_csv)\n",
    "print(_)\n",
    "print (\"The best model was scored %.2f\" % cv_score)\n",
    "# use trained SVC model to generate predictions\n",
    "# Don't archive the files or change the file names for the automated grading.\n",
    "# Do not shuffle the test dataset\n",
    "\n",
    "if submission:\n",
    "    print('Inference not done using this KFOLD run')\n",
    "    print('This is just a proof of concept to show that code can run - Trained on a subset of the initial data')\n",
    "    print('Stored prediction file contains inference when training models on full dataset')\n",
    "\n",
    "if not predictions_saved:\n",
    "    predictions = predict(testing_csv, trained_model, x)\n",
    "    output_results(predictions)\n",
    "# 3. Upload your Python code, the predictions.txt as well as a report to Collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:14:08.170901Z",
     "start_time": "2019-10-28T16:14:08.165916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 10000})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T02:36:39.671016Z",
     "start_time": "2019-10-16T02:36:39.538802Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = df['C'], y = df['cv_train_accuracy'])\n",
    "sns.lineplot(x = df['C'], y = df['cv_test_accuracy'])\n",
    "plt.ylabel('cv_accuracy')\n",
    "plt.legend(['train_acc', 'test_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:39:58.466898Z",
     "start_time": "2019-10-28T16:39:10.238936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data\n",
      "removing outliers\n",
      "Categorical one hot encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdine\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_csv = \"salary.labeled.csv\"\n",
    "testing_csv = \"salary.2Predict.csv\"\n",
    "x,y, xcols = load_data(training_csv, encode_cat = True, outlier_removal = True, \n",
    "                               feature_engineering = True, down_sample= False)\n",
    "x = pd.DataFrame(MinMaxScaler().fit_transform(x) , columns = x.columns)\n",
    "\n",
    "model = SVC(C = 100)\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:39:59.299670Z",
     "start_time": "2019-10-28T16:39:59.243819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data\n",
      "Categorical one hot encoding\n"
     ]
    }
   ],
   "source": [
    "x_test,y_test, xcols = load_data(testing_csv, encode_cat = True, outlier_removal = False, \n",
    "                               feature_engineering = True, down_sample= False)\n",
    "x_test = pd.DataFrame(MinMaxScaler().fit_transform(x_test) , columns = xcols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:40:05.518031Z",
     "start_time": "2019-10-28T16:40:00.020740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data\n",
      "Categorical one hot encoding\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(testing_csv, model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:40:07.940161Z",
     "start_time": "2019-10-28T16:40:07.935194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 9677, 1: 323})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T17:19:54.606086Z",
     "start_time": "2019-10-28T17:19:54.575145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'<=50K': 7937, '>50K': 2063})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt('predictions1.txt', dtype = 'str')\n",
    "Counter(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
